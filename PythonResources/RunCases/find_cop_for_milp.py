#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Apr 11 11:50:10 2025

@author: casper

Known issue:
    The mat file is too big for BuildingsPy (or DyMat, or SDF) to load
        on some machines.
    Need to manually use the command generated by `generate_dymola_command()`
        in Dymola to trim down the mat file, leaving only desired variables
        in the csv file.
    This step is not automated yet.
"""

import os
#import glob
import pandas as pd
import numpy as np
#from buildingspy.io.outputfile import Reader

#CWD = os.getcwd()
CWD = os.path.dirname(os.path.abspath(__file__))
mat_file_name = os.path.join(CWD, "simulations", "2025-05-05-simulations", "detailed_plant_five_hubs_futu", "DetailedPlantFiveHubs.mat")
csv_file_name = os.path.join(CWD, "simulations", "2025-05-05-simulations", "detailed_plant_five_hubs_futu", "DetailedPlantFiveHubs.csv")

PRINT_RESULTS = False
WRITE_TO_XLSX = True
PATH_XLSX = os.path.join(CWD, "cop_for_milp.xlsx")
nBui = 5

# remarks to be written to the output file
WRITE_REMARKS = True

def get_commit_hash():
    import git
    repo = git.Repo(search_parent_directories=True)
    sha = repo.head.object.hexsha
    return sha

if WRITE_REMARKS:
    remarks = pd.DataFrame(np.array([['Model', 'ThermalGridJBA.Networks.Validation.DetailedPlantFiveHubs'],
                                     ['Weather scenario', 'fTMY'],
                                     ['Result file at commit', '343b6a5a47399dbee9441f1aaf96fb83d38b8aa6'],
                                     ['This file generated at commit', get_commit_hash()]]))

#%% Generate variable list
def generate_indexed_var_list(pre_index, holder, i):
    """ Replaces the `holder` string in `pre_index` with index `i`.
        Both `pre_index` and `i` can be either a single value or a list.
    """
    
    _pre_index = [pre_index] if isinstance(pre_index, str) else pre_index
    _i = [i] if isinstance(i, int) else i
    
    var_list = list()
    for pre, ind in [(pre, ind) for pre in _pre_index for ind in _i]:
        var_list.append(pre.replace(holder,str(ind)))
        
    return var_list
    
index_holder = r'%%i%%' # placeholder string to be replaced with index

# keys are the var names to be read from the result file
# values are their corresponding short nicknames
var_dict_pre_index = {
    f'bui[{index_holder}].ets.chi.chi.COP'       : 'COP',      # not directly used for taking average
    f'bui[{index_holder}].ets.chi.uCoo'          : 'uCoo',
    f'bui[{index_holder}].ets.chi.uHea'          : 'uHea',
    f'bui[{index_holder}].ets.chi.chi.QCon_flow' : 'QCon',     # chiller condenser heat, W
    f'bui[{index_holder}].ets.chi.chi.P'         : 'PChi',     # chiller electric input, W
    f'bui[{index_holder}].ets.chi.senTEvaEnt.T'  : 'TEvaEnt',  # K
    f'bui[{index_holder}].ets.chi.senTEvaLvg.T'  : 'TEvaLvg',  # K
    f'bui[{index_holder}].ets.chi.senTConEnt.T'  : 'TConEnt',  # K
    f'bui[{index_holder}].ets.chi.senTConLvg.T'  : 'TConLvg'   # K
    }

var_list_pre_index = list(var_dict_pre_index.keys())

var_list = generate_indexed_var_list(var_list_pre_index, index_holder, range(1,nBui+1))

#%% Generate Dymola command to export csv from large mat file
def generate_dymola_command(var_list, mat_file_path, csv_file_path):
    
    s = ''
    s += f'DataFiles.convertMATtoCSV("{mat_file_path}", '
    s += '{"'
    s += '","'.join(var_list)
    s += '"}, '
    s += f'"{csv_file_path}");'
        
    return s    
    # __ref = r'DataFiles.convertMATtoCSV("/home/casper/gitRepo/thermal-grid-jba/PythonResources/RunCases/simulations/2025-05-05-simulations/detailed_plant_five_hubs_futu/DetailedPlantFiveHubs.mat", {"bui[1].ets.chi.chi.COP","bui[1].ets.chi.uCoo"}, "/home/casper/gitRepo/thermal-grid-jba/PythonResources/RunCases/simulations/2025-05-05-simulations/detailed_plant_five_hubs_futu/trimmed.csv");'

dymola_command = generate_dymola_command(var_list,
                                         mat_file_name,
                                         csv_file_name)

#%% Read exported result csv file
result_full = pd.read_csv(csv_file_name, header = 0)
    
# Convert the timestamp to datetime format
result_full['datetime'] = pd.to_datetime(result_full['Time'], unit='s', origin='2025-01-01')

#%% Crunch data for each building index

if WRITE_TO_XLSX:
    w = pd.ExcelWriter(PATH_XLSX, engine='xlsxwriter')

month_order = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
all_sums = {}
for i in range(1,nBui+1):
#for i in [1]:
    # initialise list
    var_list_bui = ['Time', 'datetime'] # initialise
    
    # indexed var names for this building
    var_list_bui += generate_indexed_var_list(var_list_pre_index, index_holder, i)
    result_bui = result_full[var_list_bui]
    
    # dict for renaming pd columns
    var_dict_indexed = {key.replace(index_holder, str(i)): value for key, value in var_dict_pre_index.items()}
    result_bui = result_bui.rename(columns=var_dict_indexed)
    
    # Filter
    result_bui = result_bui[result_bui['COP'] > 0.01] # only when chiller on
    result_bui = result_bui[result_bui['COP'] < 15.0] # remove transient at initialisation
    result_bui = result_bui[np.isclose(result_bui['Time'] % 3600, 0)] # only keep hourly sampled values
    result_bui = result_bui.iloc[:-1] # drop the last point which would be categorised to the next year
    
    # Section the data to each calendar month
    result_bui['month'] = result_bui['datetime'].dt.to_period('M')
    
    # Filter data based on operational modes
    conditions = [
        (result_bui['uCoo'] == 1) & (result_bui['uHea'] == 1),
        (result_bui['uCoo'] == 1) & (result_bui['uHea'] == 0),
        (result_bui['uCoo'] == 0) & (result_bui['uHea'] == 1)
                  ]
    modes = ['simultaneous', 'coolingonly', 'heatingonly']
    result_bui['mode'] = np.select(conditions, modes, default='other')
    
    # Monthly
    grouped = result_bui.groupby(['month', 'mode'])
    
    cop_mon_results = []
    for (month, mode), group in grouped:
        QCon_sum = group['QCon'].sum()
        PChi_sum = group['PChi'].sum()
        
        if (month, mode) not in all_sums:
            all_sums[(month, mode)] = {'QCon': 0, 'PChi': 0}
        all_sums[(month, mode)]['QCon'] += QCon_sum
        all_sums[(month, mode)]['PChi'] += PChi_sum
        
        if PChi_sum != 0:
            COP_mon = QCon_sum / PChi_sum
        else:
            COP_mon = np.nan
        
        TEvaEnt_avg = group['TEvaEnt'].mean() - 273.15
        TEvaLvg_avg = group['TEvaLvg'].mean() - 273.15
        TConEnt_avg = group['TConEnt'].mean() - 273.15
        TConLvg_avg = group['TConLvg'].mean() - 273.15
        duration = len(group)
        
        cop_mon_results.append((month, mode, COP_mon, TEvaEnt_avg, TEvaLvg_avg, TConEnt_avg, TConLvg_avg, duration))
    
    column_names = ['COP_h', 'TEvaEnt_avg|C', 'TEvaLvg_avg|C', 'TConEnt_avg|C', 'TConLvg_avg|C', 'Duration|h']
    
    cop_mon_df = pd.DataFrame(cop_mon_results, columns=['month', 'mode'] + column_names)
    
    # Convert the 'month' column to abbreviated month names and order it
    cop_mon_df['month'] = cop_mon_df['month'].dt.strftime('%b')
    cop_mon_df['month'] = pd.Categorical(cop_mon_df['month'], categories=month_order, ordered=True)
    
    # Pivot the DataFrame
    cop_mon_df_pivot = cop_mon_df.pivot_table(
        index='month',
        columns='mode',
        values=column_names,
        aggfunc='first'
    ).swaplevel(axis=1).sort_index(axis=1)
    
    # Annual
    cop_ann_results = []
    for mode in modes:
        QCon_sum = result_bui[result_bui['mode'] == mode]['QCon'].sum()
        PChi_sum = result_bui[result_bui['mode'] == mode]['PChi'].sum()
        
        if PChi_sum != 0:
            COP_ann = QCon_sum / PChi_sum
        else:
            COP_ann = np.nan
        
        TEvaEnt_avg = result_bui[result_bui['mode'] == mode]['TEvaEnt'].mean() - 273.15
        TEvaLvg_avg = result_bui[result_bui['mode'] == mode]['TEvaLvg'].mean() - 273.15
        TConEnt_avg = result_bui[result_bui['mode'] == mode]['TConEnt'].mean() - 273.15
        TConLvg_avg = result_bui[result_bui['mode'] == mode]['TConLvg'].mean() - 273.15
        duration = len(result_bui[result_bui['mode'] == mode])
        
        cop_ann_results.append((mode, COP_ann, TEvaEnt_avg, TEvaLvg_avg, TConEnt_avg, TConLvg_avg, duration))
    
    cop_ann_df = pd.DataFrame(cop_ann_results, columns=['mode'] + column_names)
    
    if PRINT_RESULTS:
        print(f"Results for ETS #{i}:")
        print("Monthly COP:")
        print(cop_mon_df_pivot)
        print("\nAnnual COP:")
        print(cop_ann_df)
    
    if WRITE_TO_XLSX:
        sheet_name = f'ETS_{i}'
        cop_mon_df_pivot.to_excel(w, sheet_name=f'{sheet_name}_monthly', index=True)
        cop_ann_df.to_excel(w, sheet_name=f'{sheet_name}_annual', index=False)

# Compute all-building COP for each month and mode
cop_all_results = []
for (month, mode), sums in all_sums.items():
    QCon_all_sum = sums['QCon']
    PChi_all_sum = sums['PChi']
    
    if PChi_all_sum != 0:
        COP_all = QCon_all_sum / PChi_all_sum
    else:
        COP_all = np.nan
    
    cop_all_results.append((month, mode, COP_all))

column_names = ['COP_h']

cop_all_df = pd.DataFrame(cop_all_results, columns=['month', 'mode'] + column_names)

# Convert the 'month' column to abbreviated month names
cop_all_df['month'] = cop_all_df['month'].dt.strftime('%b')
cop_all_df['month'] = pd.Categorical(cop_all_df['month'], categories=month_order, ordered=True)

# Pivot the DataFrame
cop_all_df_pivot = cop_all_df.pivot_table(
    index='month',
    columns='mode',
    values=column_names,
    aggfunc='first'
).swaplevel(axis=1).sort_index(axis=1)

if PRINT_RESULTS:
    print("COP across all buildings:")
    print(cop_all_df_pivot)

if WRITE_TO_XLSX:
    cop_all_df_pivot.to_excel(w, sheet_name='All buildings', index=True)
    if WRITE_REMARKS:
        remarks.to_excel(w, sheet_name="remarks", index=False)
    w.close()
    print(f"Results wrote to {PATH_XLSX}.")